# Import the Tokenizer and pad_sequences directly from tensorflow.keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing import text, sequence
# Import to_categorical for handling categorical data
from tensorflow.keras.utils import to_categorical

# Keras backend and layers
from tensorflow.keras import backend as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Lambda

# For distance calculation and data handling
from sklearn.metrics.pairwise import euclidean_distances
import numpy as np
import pandas as pd


data = """Deep learning (also known as deep structured learning) is part of a
broader family of machine learning methods based on artificial neural networks
with representation learning. Learning can be supervised, semi-supervised or unsupervised.
Deep-learning architectures such as deep neural networks, deep belief networks,
deep reinforcement learning, recurrent neural networks, convolutional neural networks and
Transformers have been applied to fields including computer vision, speech recognition,
natural language processing, machine translation, bioinformatics, drug design,
medical image analysis, climate science, material inspection and board game programs,
where they have produced results comparable to and in some cases surpassing human expert performance.
"""
dl_data = data.split()


tokenizer=Tokenizer()
tokenizer.fit_on_texts(dl_data)
word2id=tokenizer.word_index
word2id['PAD']=0
id2word={i:w for w,i in word2id.items()}

wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]

vocab_size=len(word2id)
embed_size=100
window_size=2
print("Vocabulary size: ", vocab_size)
print("Vocabulary Sample: ", list(word2id.items())[:10])


def generate_context_word_pairs(corpus, window_size, vocab_size):
    context_length = window_size*2
    for words in corpus:
        sentence_length = len(words)
        for index, word in enumerate(words):
            context_words = []
            label_word   = []
            start = index - window_size
            end = index + window_size + 1
            context_words.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])
            label_word.append(word)
            x=sequence.pad_sequences(context_words, maxlen=context_length)
            y=to_categorical(label_word, vocab_size)
            yield (x,y)


cbow=Sequential()
cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))
cbow.add(Lambda(lambda x:K.mean(x, axis=1), output_shape=(embed_size,)))
cbow.add(Dense(vocab_size, activation="softmax"))
cbow.compile(loss="categorical_crossentropy", optimizer="rmsprop")
print(cbow.summary())


for epoch in range(1,6):
    loss=0
    i=0

    for x,y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):
        i+=1
        loss+=cbow.train_on_batch(x,y)
        if i % 100000 == 0:
            print('Processed {} (context, word) pairs'.format(i))

    print('Epoch:',epoch, '\tLoss:', loss)
    print()


weights=cbow.get_weights()[0]
weights=weights[1:]
print(weights.shape)
pd.DataFrame(weights, index=list(id2word.values())[1:]).head()
#

distance_matrix=euclidean_distances(weights)
print(distance_matrix.shape)
inwords =input()
similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[0:6]]
                   for search_term in {inwords}}

similar_words

